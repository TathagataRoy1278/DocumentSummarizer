{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "recquired_length = 0.25\n",
    "total_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "def load_word_embeddings():\n",
    "    f = open(\"/media/tatan/A0A04E3AA04E16E6/word_vectors/glove.6B.100d.txt\",encoding = 'utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = coefs\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_word_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(\"https://web.uvic.ca/~sdoyle/E302/Notes/SummaryNotes.html\").text\n",
    "soup = BeautifulSoup(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.find('title').text\n",
    "paras = soup.findAll('p')\n",
    "text = [i.text for i in paras if len(i.text)>70]\n",
    "text = \"\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = sent_tokenize(text)\n",
    "total_length = len(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencevector_avg(word_vectors):\n",
    "    avg_vector = []\n",
    "    if len(word_vectors)!=0:\n",
    "        avg_vector = np.array([sum([vector[axis] for vector in word_vectors])/len(word_vectors) for axis in range(len(word_vectors[0]))])\n",
    "    else:\n",
    "        avg_vector = np.zeros((100,))\n",
    "    return avg_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencevector_avg_weighted(word_vectors,tmp_vectors,weight):\n",
    "    for i in range(len(tmp_vectors)):\n",
    "        for j in range(len(tmp_vectors[0])):\n",
    "            tmp_vectors[i][j] = weight*tmp_vectors[i][j]\n",
    "    word_vectors+=tmp_vectors\n",
    "    avg_vector = []\n",
    "    if len(word_vectors)!=0:\n",
    "        avg_vector = np.array([sum([vector[axis] for vector in word_vectors])/len(word_vectors) for axis in range(len(word_vectors[0]))])\n",
    "    else:\n",
    "        avg_vector = np.zeros((100,))\n",
    "    return avg_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_and_punctuations(sen):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sen = \" \".join(tokenizer.tokenize(sen))\n",
    "    new_vec = \" \".join([word for word in sen.split() if word not in stop_words])\n",
    "    return new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vectors(sentence):\n",
    "    sentence = remove_stopwords_and_punctuations(sentence)\n",
    "    word_vectors = []\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            word_vectors.append(deepcopy(word_embeddings[word.lower()]))\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummary(title,sentence_tokens):\n",
    "    summary = []\n",
    "    sumoftmps = 9\n",
    "    avg = 0\n",
    "    ctr = 1\n",
    "    title_vector = sentencevector_avg(get_word_vectors(title))\n",
    "    for sentence_token in sentence_tokens:\n",
    "        sentence_vector = []\n",
    "\n",
    "        sentence_vector = sentencevector_avg(get_word_vectors(sentence_token))\n",
    "        \n",
    "        tmp = pearsonr(sentence_vector,title_vector)[0]\n",
    "        sumoftmps+=tmp\n",
    "        avg = sumoftmps/ctr\n",
    "        ctr+=1\n",
    "        summary.append((tmp,sentence_token))\n",
    "        if(tmp>=avg):\n",
    "            title_vector = sentencevector_avg_weighted(get_word_vectors(title),get_word_vectors(sentence_token),0.3)\n",
    "            \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'sciforce'\n",
      "'sciforce'\n",
      "'summarizers'\n",
      "'luhl'\n",
      "'abstractive'\n",
      "'abstractive'\n",
      "'abstractive'\n",
      "'summarizers'\n",
      "'summarizer'\n",
      "'tfidf'\n",
      "'tfidf'\n",
      "'tfidf'\n",
      "'tfidf'\n",
      "'tfidf'\n",
      "'tfidf'\n",
      "'tfidf'\n",
      "[(4, (0.7446782345233126, 'An important research of these days introduced a method to extract salient sentences from the text using features such as word and phrase frequency.')), (5, (0.7864292280168058, 'In this work, Luhl proposed to weight the sentences of a document as a function of high frequency words, ignoring very high frequency common words –the approach that became the one of the pillars of NLP.By now, the whole branch of natural language processing dedicated to summarization emerged, covering a variety of tasks:The approaches to text summarization vary depending on the number of input documents (single or multiple), purpose (generic, domain specific, or query-based) and output (extractive or abstractive).Extractive summarization means identifying important sections of the text and generating them verbatim producing a subset of the sentences from the original text; while abstractive summarization reproduces important material in a new way after interpretation and examination of the text using advanced natural language techniques to generate a new shorter text that conveys the most critical information from the original one.Obviously, abstractive summarization is more advanced and closer to human-like interpretation.')), (6, (0.7040704180035543, 'Though it has more potential (and is generally more interesting for researchers and developers), so far the more traditional methods have proved to yield better results.That is why in this blog post we’ll give a short overview of such traditional approaches that have beaten a path to advanced deep learning techniques.By now, the core of all extractive summarizers is formed of three independent tasks:There are two types of representation-based approaches: topic representation and indicator representation.')), (8, (0.7339219850128117, 'The techniques used for this differ in terms of their complexity, and are divided into frequency-driven approaches, topic word approaches, latent semantic analysis and Bayesian topic models.')), (16, (0.7226173177954578, 'The two most common techniques in this category are: word probability and TFIDF (Term Frequency Inverse Document Frequency).')), (19, (0.7111559232546044, 'TFIDF, a more sophisticated technique, assesses the importance of words and identifies very common words (that should be omitted from consideration) in the document(s) by giving low weights to words appearing in most documents.')), (22, (0.7548591876144638, 'Afterwards, the centroids are used to identify sentences in each cluster that are central to the topic.Latent semantic analysis (LSA) is an unsupervised method for extracting a representation of text semantics based on observed words.')), (37, (0.754307035039461, 'At the same time, such measuring only of the formal side of the sentence structure without the syntactic and semantic information limits the application of the method.Machine learning approaches that treat summarization as a classification problem are widely used now trying to apply Naive Bayes, decision trees, support vector machines, Hidden Markov models and Conditional Random Fields to obtain a true-to-life summary.')), (38, (0.7613577691366687, 'As it has turned out, the methods explicitly assuming the dependency between sentences (Hidden Markov model and Conditional Random Fields) often outperform other techniques.Yet, the problem with classifiers is that if we utilize supervised learning methods for summarization, we need a set of labeled documents to train the classifier, meaning development of a corpus.')), (39, (0.7677138444290561, 'A possible way-out is to apply semi-supervised approaches that combine a small amount of labeled data along with a large amount of unlabeled data in training.Overall, machine learning methods have proved to be very effective and successful both in single and multi-document summarization, especially in class-specific summarization such as drawing scientific paper abstracts or biographical summaries.Though abundant, all the summarization methods we have mentioned could not produce summaries that would similar to human-created summaries.'))]\n",
      "An important research of these days introduced a method to extract salient sentences from the text using features such as word and phrase frequency.\n",
      "In this work, Luhl proposed to weight the sentences of a document as a function of high frequency words, ignoring very high frequency common words –the approach that became the one of the pillars of NLP.By now, the whole branch of natural language processing dedicated to summarization emerged, covering a variety of tasks:The approaches to text summarization vary depending on the number of input documents (single or multiple), purpose (generic, domain specific, or query-based) and output (extractive or abstractive).Extractive summarization means identifying important sections of the text and generating them verbatim producing a subset of the sentences from the original text; while abstractive summarization reproduces important material in a new way after interpretation and examination of the text using advanced natural language techniques to generate a new shorter text that conveys the most critical information from the original one.Obviously, abstractive summarization is more advanced and closer to human-like interpretation.\n",
      "Though it has more potential (and is generally more interesting for researchers and developers), so far the more traditional methods have proved to yield better results.That is why in this blog post we’ll give a short overview of such traditional approaches that have beaten a path to advanced deep learning techniques.By now, the core of all extractive summarizers is formed of three independent tasks:There are two types of representation-based approaches: topic representation and indicator representation.\n",
      "The techniques used for this differ in terms of their complexity, and are divided into frequency-driven approaches, topic word approaches, latent semantic analysis and Bayesian topic models.\n",
      "The two most common techniques in this category are: word probability and TFIDF (Term Frequency Inverse Document Frequency).\n",
      "TFIDF, a more sophisticated technique, assesses the importance of words and identifies very common words (that should be omitted from consideration) in the document(s) by giving low weights to words appearing in most documents.\n",
      "Afterwards, the centroids are used to identify sentences in each cluster that are central to the topic.Latent semantic analysis (LSA) is an unsupervised method for extracting a representation of text semantics based on observed words.\n",
      "At the same time, such measuring only of the formal side of the sentence structure without the syntactic and semantic information limits the application of the method.Machine learning approaches that treat summarization as a classification problem are widely used now trying to apply Naive Bayes, decision trees, support vector machines, Hidden Markov models and Conditional Random Fields to obtain a true-to-life summary.\n",
      "As it has turned out, the methods explicitly assuming the dependency between sentences (Hidden Markov model and Conditional Random Fields) often outperform other techniques.Yet, the problem with classifiers is that if we utilize supervised learning methods for summarization, we need a set of labeled documents to train the classifier, meaning development of a corpus.\n",
      "A possible way-out is to apply semi-supervised approaches that combine a small amount of labeled data along with a large amount of unlabeled data in training.Overall, machine learning methods have proved to be very effective and successful both in single and multi-document summarization, especially in class-specific summarization such as drawing scientific paper abstracts or biographical summaries.Though abundant, all the summarization methods we have mentioned could not produce summaries that would similar to human-created summaries.\n",
      " 3812\n"
     ]
    }
   ],
   "source": [
    "summary = [(i,s) for i,s in enumerate(getSummary(title,sentence_tokens))]\n",
    "summary.sort(key = lambda x: x[1][0], reverse = True)\n",
    "\n",
    "final_summary = sorted(summary[:int(recquired_length*total_length)],key = lambda x: x[0])\n",
    "print(final_summary)\n",
    "summary = \"\"\n",
    "for i in final_summary:\n",
    "    summary+=i[1][1]+\"\\n\"\n",
    "print(summary,len(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
