{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "recquired_length = 0.33333\n",
    "total_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "def load_word_embeddings():\n",
    "    f = open(\"/media/tatan/A0A04E3AA04E16E6/word_vectors/glove.6B.100d.txt\",encoding = 'utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = coefs\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_word_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(\"https://web.uvic.ca/~sdoyle/E302/Notes/SummaryNotes.html\").text\n",
    "soup = BeautifulSoup(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.find('title').text\n",
    "paras = soup.findAll('p')\n",
    "text = [i.text for i in paras if len(i.text)>70]\n",
    "text = \"\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = sent_tokenize(text)\n",
    "total_length = len(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencevector_avg(word_vectors):\n",
    "    avg_vector = []\n",
    "    if len(word_vectors)!=0:\n",
    "        avg_vector = np.array([sum([vector[axis] for vector in word_vectors])/len(word_vectors) for axis in range(len(word_vectors[0]))])\n",
    "    else:\n",
    "        avg_vector = np.zeros((300,))\n",
    "    return avg_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencevector_avg_weighted(word_vectors,tmp_vectors,weight):\n",
    "    for i in range(len(tmp_vectors)):\n",
    "        for j in range(len(tmp_vectors[0])):\n",
    "            tmp_vectors[i][j] = weight*tmp_vectors[i][j]\n",
    "    word_vectors+=tmp_vectors\n",
    "    avg_vector = []\n",
    "    if len(word_vectors)!=0:\n",
    "        avg_vector = np.array([sum([vector[axis] for vector in word_vectors])/len(word_vectors) for axis in range(len(word_vectors[0]))])\n",
    "    else:\n",
    "        avg_vector = np.zeros((300,))\n",
    "    return avg_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_and_punctuations(sen):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sen = \" \".join(tokenizer.tokenize(sen))\n",
    "    new_vec = \" \".join([word for word in sen.split() if word not in stop_words])\n",
    "    return new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vectors(sentence):\n",
    "    sentence = remove_stopwords_and_punctuations(sentence)\n",
    "    word_vectors = []\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            word_vectors.append(word_embeddings[word.lower()])\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummary(title,sentence_tokens):\n",
    "    summary = []\n",
    "    title_vector = sentencevector_avg(get_word_vectors(title))\n",
    "    for sentence_token in sentence_tokens:\n",
    "        sentence_vector = []\n",
    "\n",
    "        sentence_vector = sentencevector_avg(get_word_vectors(sentence_token))\n",
    "        \n",
    "        tmp = pearsonr(sentence_vector,title_vector)[0]\n",
    "        summary.append((tmp,sentence_token))\n",
    "        if tmp>0.5:\n",
    "            pass\n",
    "            #title_vector = sentencevector_avg_weighted(nlp(title),nlp(sentence_token),0.5)\n",
    "            #title_vector = sentencevector_avg_weighted(nlp(sentence_token),nlp(\"hell\"),0)\n",
    "            \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'qmp'\n",
      "'qmp'\n"
     ]
    }
   ],
   "source": [
    "summary = [(i,s) for i,s in enumerate(getSummary(title,sentence_tokens))]\n",
    "summary.sort(key = lambda x: x[1][0], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = sorted(summary[:int(recquired_length*total_length)],key = lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the most highly valued skills in any workplace that generates a lot of words—whether written or spoken—is the ability to effectively summarize that information into a more concise and readable form.\\nAdministrators may deal with reports, proposals, policy documents and briefing notes that together easily reach hundreds of pages.\\nYou may have to write an executive summary of a report, describe how a project is going, convey the recommendations arising from a lengthy study.\\nThis summary is about 20% of the original because the original itself is short.\\nWith a longer original, a summary might be 5% or less.\\nThe language of the summary puts the original ideas into new words.\\nThe important information usually includes controlling ideas (purpose statements and topic sentences), major findings, and conclusions or recommendations.It usually doesn't include any of the following: non-essential background information; the author's personal comments or conjectures; introductions; long explanations, examples, or definitions; visuals; or data of questionable accuracy.People read summaries to get the information they need as efficiently as possible.\\nIn a large document, the summary may be the only part a reader actually reads.\\nThink of your summary as a highly condensed version of the source document.\\nA reader should be able to read, understand and find the essential meaning by reading your summary.\\nAvoid adding comments or modifiers that add meaning that was not in the original (e.g.\\n\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = \"\"\n",
    "for i in final_summary:\n",
    "    summary+=i[1][1]+\"\\n\"\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,2,3,4,5,6,7,6]*10:\n",
    "    print(nlp(\":\")[0].vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tmp1)):\n",
    "    if tmp1[i][0]!=tmp2[i][0]:\n",
    "        print(i,sentence_tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.vector for token in tmp3[0] if not token.is_punct][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token for token in nlp(sentence_tokens[0]) if not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp(\"of\")[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
